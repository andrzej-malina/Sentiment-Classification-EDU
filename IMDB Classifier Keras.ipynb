{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ajankowski\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26910</th>\n",
       "      <td>I found this episode to be one of funniest I'v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>This movie has it all. Great actors, good dial...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7020</th>\n",
       "      <td>I'm Egyptian. I have a green card. I have been...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>THE CELL fascinated me at first glance. I was ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35164</th>\n",
       "      <td>Crush provides a combination of drama, humor a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12690</th>\n",
       "      <td>Like a lot of the comments above me, also I th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24490</th>\n",
       "      <td>Wow -- this movie was really bad! You talk abo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>Nazarin is some kind of saint,he wants to live...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27333</th>\n",
       "      <td>It is not every film's job to stimulate you su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7765</th>\n",
       "      <td>I absolutely adored this movie. For me, the be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "26910  I found this episode to be one of funniest I'v...          1\n",
       "1529   This movie has it all. Great actors, good dial...          1\n",
       "7020   I'm Egyptian. I have a green card. I have been...          1\n",
       "4738   THE CELL fascinated me at first glance. I was ...          1\n",
       "35164  Crush provides a combination of drama, humor a...          1\n",
       "12690  Like a lot of the comments above me, also I th...          0\n",
       "24490  Wow -- this movie was really bad! You talk abo...          0\n",
       "6722   Nazarin is some kind of saint,he wants to live...          1\n",
       "27333  It is not every film's job to stimulate you su...          1\n",
       "7765   I absolutely adored this movie. For me, the be...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data\n",
    "df = pd.read_csv('imdb_data', encoding = 'utf-8')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train and test datasets\n",
    "X_train = df.loc[:34999, 'review'].values\n",
    "y_train = df.loc[:34999, 'sentiment'].values\n",
    "X_test = df.loc[15000:, 'review'].values\n",
    "y_test = df.loc[15000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x1fa5d2921d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word embeddings with Keras Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "total_reviews = X_train + X_test\n",
    "tokenizer.fit_on_texts(total_reviews)\n",
    "\n",
    "#pad sequences\n",
    "review_lengths = [len(s.split()) for s in total_reviews]\n",
    "#pad_length = int(np.mean(review_lengths))\n",
    "pad_length = 100\n",
    "\n",
    "#define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=pad_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=pad_length, padding='post')\n",
    "\n",
    "\n",
    "#embedding layer\n",
    "embedding_dim =100 #dimension of vector\n",
    "Embedding(vocab_size, embedding_dim, input_length=pad_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ajankowski\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ajankowski\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          12613700  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 12,626,501\n",
      "Trainable params: 12,626,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=pad_length),\n",
    "    GRU(units=32, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ajankowski\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 35000 samples, validate on 35000 samples\n",
      "Epoch 1/10\n",
      " - 93s - loss: 0.5232 - acc: 0.7411 - val_loss: 0.3980 - val_acc: 0.8347\n",
      "Epoch 2/10\n",
      " - 89s - loss: 0.3379 - acc: 0.8628 - val_loss: 0.3634 - val_acc: 0.8519\n",
      "Epoch 3/10\n",
      " - 92s - loss: 0.2442 - acc: 0.9070 - val_loss: 0.3441 - val_acc: 0.8643\n",
      "Epoch 4/10\n",
      " - 92s - loss: 0.1850 - acc: 0.9326 - val_loss: 0.3554 - val_acc: 0.8713\n",
      "Epoch 5/10\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=10, verbose=2, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing on some examples\n",
    "sample_1 = 'This movie is fantastic! I really like it, because it is so exciting'\n",
    "sample_2 = 'Very nice movie.'\n",
    "sample_3 = 'What a bad movie that was. I would expect something much better.'\n",
    "sample_4 = 'This movie really sucks. No action and plot is flat like table.'\n",
    "sample_5 = 'Very interesting cinema. Great play of the actors.'\n",
    "\n",
    "test_samples = [sample_1, sample_2, sample_3, sample_4, sample_5]\n",
    "test_samples_tokens = tokenizer.texts_to_sequences(test_samples)\n",
    "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=pad_length)\n",
    "\n",
    "#predict\n",
    "model.predict(x=test_samples_tokens_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step is to prepare the text corpus for learning the embedding by creating word tokens, removing punctuation, \n",
    "#removing stop words etc. The word2vec algorithm processes documents sentence by sentence.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "review_lines = list()\n",
    "lines = df['review'].values.tolist()\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    tokens = tokenizer.tokenize(line)\n",
    "    #convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    #remove tokens that are not alphabetic\n",
    "    tokens = [t for t in tokens if t.isalpha]\n",
    "    #filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    review_lines.append(tokens)\n",
    "    \n",
    "len(review_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=embedding_dim, window=5, workers=4, min_count=1)\n",
    "\n",
    "#vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print ('Vocabulary size is: {}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.doesnt_match('dog cat pig actor'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.wv.save_word2vec_format('w2v_imdb_model', binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pre-trained Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the word embedding from file\n",
    "embeddings_index ={}\n",
    "f = open(os.path.join('','w2v_imdb_model'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert word embedding into tokenized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the text samples into a 2D integer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(review_lines)\n",
    "sequences = tokenizer.texts_to_sequences(review_lines)\n",
    "\n",
    "#pad sequences\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))\n",
    "review_pad = pad_sequences(sequences, maxlen=pad_length)\n",
    "sentiment = df['sentiment'].values\n",
    "print('Shape of review sensor: ', review_pad.shape)\n",
    "print('Shape of sentiment sensor: ', sentiment.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will map embeddings from the loaded word2vec model for each word to the tokenizer.word_index vocabulary \n",
    "#and create a matrix with  word vectors.\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words: \n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #words not found in embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are now ready with the trained embedding vector to be used directly in the embedding layer. \n",
    "#In the below code, the only change from previous model is using the embedding_matrix as input to the Embedding layer \n",
    "#and setting trainable = False, since the embedding is already learned.\n",
    "embedding_layer = Embedding(num_words, \n",
    "                            embedding_dim, \n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=pad_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential([\n",
    "    embedding_layer,\n",
    "    GRU(units=32, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training set and validation set\n",
    "validation_split_ratio = 0.3\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(validation_split_ratio*review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]\n",
    "\n",
    "print('Shape of X_train_pad tensor: ', X_train_pad.shape)\n",
    "print('Shape of y_train tensor: ', y_train.shape)\n",
    "print('Shape of X_test_pad tensor: ', X_test_pad.shape)\n",
    "print('Shape of y_test tensor: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=10, verbose=2, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
